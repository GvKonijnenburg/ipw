{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f5a74f",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = r'C:/Git/HonoursProject/ipw-clusterer/ipw_clusterer/src'\n",
    "DATA_PATH = r'C:/Git/HonoursProject/ipw-clusterer/data/'\n",
    "\n",
    "#Show Exploratory Data Analyis?\n",
    "EDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf79a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add source python files to project\n",
    "import sys\n",
    "sys.path.insert(0, CODE_PATH) \n",
    "import ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import re\n",
    "#import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = 'log.txt', level = logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8b617",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e66279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ipw.parse.read(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c774a",
   "metadata": {},
   "source": [
    "## Nr of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    print(f'Total records: {len(df)}')\n",
    "    print('Records per status:')\n",
    "    print(df['status'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0caa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select closed records\n",
    "df = df[df['status'] == 'closed']\n",
    "df.drop('status', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd9bb8",
   "metadata": {},
   "source": [
    "## Clean the text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da89001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ipw.parse.clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2e8d",
   "metadata": {},
   "source": [
    "# Exploratory Data Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b100c",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84514cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the length of each string field in words per record  \n",
    "if EDA:\n",
    "    df_stats = df.apply(lambda x: x.fillna('').str.split().apply(len)) \n",
    "    summary_stats = df_stats.describe()\n",
    "    for col in df_stats.columns:\n",
    "        summary_stats.loc['empty', col] = df_stats[col].value_counts(sort = False).get(0, 0)\n",
    "        summary_stats.loc['not_empty', col] = summary_stats.loc['count', col] - summary_stats.loc['empty', col]\n",
    "    display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ffca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box and whisker plot\n",
    "if EDA:\n",
    "    plt.boxplot(df_stats)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xticks(range(1, len(df_stats.columns) + 1), df_stats.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c2575",
   "metadata": {},
   "source": [
    "## Unique words and occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    words_dict = {}\n",
    "\n",
    "    for column in df:\n",
    "        for index, row in df.iterrows():\n",
    "                words_dict = ipw.text.add_word_count_to_dict(row[column], words_dict)\n",
    "\n",
    "    total_words = sum(words_dict.values())\n",
    "    print(f'Total words: {total_words}')\n",
    "    print(f'Unique words: {len(words_dict)}')\n",
    "    print(f'Average occurance: {total_words / len(words_dict):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7035edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    ipw.plots.wordcloud(words_dict).show()\n",
    "    ipw.plots.histogram_wordfreq(words_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d07e1",
   "metadata": {},
   "source": [
    "# Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cedbaa",
   "metadata": {},
   "source": [
    "## Nouns in description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_1 = df['description'].apply(ipw.text.filter).to_frame('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d01d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = {}\n",
    "\n",
    "for index, row in df_model_1.iterrows():\n",
    "    obs_dict = ipw.text.add_word_count_to_dict(row['text'], obs_dict)\n",
    "     \n",
    "total_words = sum(obs_dict.values())\n",
    "print(f'Total words for model: {total_words}')\n",
    "print(f'Unique words: {len(obs_dict)}')\n",
    "print(f'Average occurance: {total_words / len(obs_dict):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw.plots.wordcloud(obs_dict, 12, 8).show()\n",
    "ipw.plots.histogram_wordfreq(obs_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cb945",
   "metadata": {},
   "source": [
    "## spaCy vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nl_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load natural language model for dutch\n",
    "nlp = nl_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of series objects representing the columns of the new DataFrame  \n",
    "vector_names = [f\"V{i}\" for i in range(nlp.vocab.vectors_length)]  \n",
    "column_list = [] \n",
    "\n",
    "for name in vector_names:\n",
    "     column_list.append(pd.Series(name=name, index=df.index, dtype=float))  \n",
    "  \n",
    "# Loop over the strings in the original DataFrame and add their spaCy vectors to the column Series objects  \n",
    "for i, text in enumerate(df_model_1['text']):  \n",
    "    doc = nlp(text)  \n",
    "    for j, value in enumerate(doc.vector):  \n",
    "        column_list[j][i] = value  \n",
    "  \n",
    "# Concatenate the column Series objects to create the new DataFrame  \n",
    "df_vector_1 = pd.concat(column_list, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad3fa5",
   "metadata": {},
   "source": [
    "## Remove samples that have empty vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the norm of each row using np.linalg.norm()  \n",
    "norms = df_vector_1.apply(lambda row: np.linalg.norm(row), axis=1)  \n",
    "\n",
    "print(f'Number of samples before selection: {len(df)}')\n",
    "# Filter out the rows where the norm is zero  \n",
    "df_vector_1 = df_vector_1[norms != 0]  \n",
    "\n",
    "print(f'Number of samples for clustering: {len(df_vector_1)}')\n",
    "\n",
    "# also remove from the original dataframe with text\n",
    "df_model_1 = df_model_1[df_model_1.index.isin(df_vector_1.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba4a1e",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de07756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    correlation = df_vector_1.corr()\n",
    "    correlation = np.triu(correlation, k=1)  # Keep only the upper triangle of the correlation matrix  \n",
    "\n",
    "    sns.heatmap(abs(correlation), center = 0, cmap=\"RdBu\", vmax = 1.0, vmin = 0.0)\n",
    "    print(f'Max correlation: {correlation.max() :.3f}')  \n",
    "    print(f'Min correlation: {correlation.min() :.3f}')  \n",
    "    print(f'Mean absolute correlation: {(abs(correlation)).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942ecc5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import AffinityPropagation, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09486e9f",
   "metadata": {},
   "source": [
    "## Set Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c633c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to be able to get repeatable results\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71f763",
   "metadata": {},
   "source": [
    "## Distance and affinity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = ipw.models.distance_matrix(df_vector_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7e95",
   "metadata": {},
   "source": [
    "## Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c63ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dampings = np.arange(0.5, 1.0, 0.05) \n",
    "af_results, af_labels, af_centers = ipw.models.affinity_propagation(dist_matrix, dampings, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76056474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(af_labels+1)):\n",
    "    print(f'Cluster {i} size: {sum(af_labels == i)}')\n",
    "    \n",
    "print('----')\n",
    "best = af_results[af_results['VRC'] == max(af_results['VRC'])]\n",
    "damping = best.index[0]\n",
    "print(f'Best results at damping factor: {damping}')\n",
    "print('----')\n",
    "print(best.iloc[0])\n",
    "ipw.plots.histogram_labels(af_labels)\n",
    "ipw.plots.silhouette(dist_matrix, af_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_label_dicts = ipw.text.words_per_label(df_model_1, af_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ff9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordclouds(label_dicts):\n",
    "    return\n",
    "\n",
    "    wordclouds = [plot_functions.wordcloud_from_dict(value, 5, 5) for value in label_dicts.values()]\n",
    "\n",
    "    # Create a grid of subplots, ncols wide or less if there are less clusters\n",
    "    ncols = min(5, n)\n",
    "    nrows = int(np.ceil(len(wordclouds) / ncols))\n",
    "    nplots = ncols * nrows\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20))  \n",
    "    keys = list(label_dicts.keys())\n",
    "\n",
    "    # Plot each individual wordcloud in a separate subplot  \n",
    "    for i in range(nplots): \n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        if nrows == 1:\n",
    "            if i < len(wordclouds):\n",
    "                index = keys.index(i)\n",
    "                axs[col].imshow(wordclouds[index].to_array(), interpolation='bilinear')  \n",
    "                axs[col].set_title(f'Cluster {keys[index]}', pad = 15)\n",
    "            axs[col].axis('off')  \n",
    "        else:\n",
    "            if i < len(wordclouds):\n",
    "                index = keys.index(i)\n",
    "                axs[row, col].imshow(wordclouds[index].to_array(), interpolation='bilinear')  \n",
    "                axs[row, col].set_title(f'Cluster {keys[index]}')\n",
    "            axs[row, col].axis('off')  \n",
    "\n",
    "\n",
    "    # Show the grid of subplots  \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1840c5",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for hyperparameter n_clusters and results\n",
    "n_clusters = range(2, 10)\n",
    "sc_results, sc_labels = ipw.models.spectral_clustering(dist_matrix, n_clusters, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44244ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(sc_labels+1)):\n",
    "    print(f'Cluster {i} size: {sum(sc_labels == i)}')\n",
    "    \n",
    "print('----')\n",
    "best = sc_results[sc_results['VRC'] == max(sc_results['VRC'])]\n",
    "n = best.index[0]\n",
    "print(f'Best results at {n} clusters')\n",
    "\n",
    "print('----')\n",
    "print(best.iloc[0])\n",
    "ipw.plots.histogram_labels(sc_labels)\n",
    "ipw.plots.silhouette(dist_matrix, sc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52424a",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for hyperparameter damping and results\n",
    "n_clusters = range(2, 20)\n",
    "ac_results, ac_labels = ipw.models.agglomerative_clustering(dist_matrix, n_clusters, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2064b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(ac_labels+1)):\n",
    "    print(f'Cluster {i} size: {sum(ac_labels == i)}')\n",
    "    \n",
    "print('----')\n",
    "best = ac_results[ac_results['VRC'] == max(ac_results['VRC'])]\n",
    "n = best.index[0]\n",
    "print(f'Best results at {n} clusters')\n",
    "\n",
    "print('----')\n",
    "print(best.iloc[0])\n",
    "ipw.plots.histogram_labels(ac_labels)\n",
    "ipw.plots.silhouette(dist_matrix, ac_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b0df",
   "metadata": {},
   "source": [
    "# DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5468c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_arr = np.arange(0.01, 0.2, 0.01) \n",
    "min_samples = range(2, 10)\n",
    "db_results, db_labels = ipw.models.dbscan(dist_matrix, eps_arr, min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b229954",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----')\n",
    "print('Cluster label -1 is the noisy data')\n",
    "print('----')\n",
    "\n",
    "for i in range(min(db_labels), max(db_labels+1)):\n",
    "    print(f'Cluster {i} size: {sum(db_labels == i)}')\n",
    "    \n",
    "print('----')\n",
    "best = db_results[db_results['VRC'] == np.nanmax(db_results['VRC'])]\n",
    "i = best.index[0]\n",
    "print(best.iloc[0])\n",
    "\n",
    "ipw.plots.histogram_labels(db_labels)\n",
    "ipw.plots.silhouette(dist_matrix, db_labels, adjustment = 1) # adjustment to allow for cluster -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
