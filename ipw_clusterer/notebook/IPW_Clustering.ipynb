{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f5a74f",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = r'C:/Git/HonoursProject/ipw-clusterer/ipw_clusterer/'\n",
    "DATA_PATH = r'C:/Git/HonoursProject/ipw-clusterer/data/'\n",
    "\n",
    "EDA = True #Show Exploratory Data Analyis\n",
    "SHOW_MODEL_PLOTS = True #Show model plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf79a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add source python files to project\n",
    "import sys\n",
    "sys.path.insert(0, CODE_PATH) \n",
    "import ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import re\n",
    "#import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = 'log.txt', level = logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8b617",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e66279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ipw.parse.read(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c774a",
   "metadata": {},
   "source": [
    "## Nr of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    print(f'Total records: {len(df)}')\n",
    "    print('Records per status:')\n",
    "    print(df['status'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0caa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select closed records\n",
    "df = df[df['status'] == 'closed']\n",
    "df.drop('status', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd9bb8",
   "metadata": {},
   "source": [
    "## Clean the text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da89001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ipw.parse.clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2e8d",
   "metadata": {},
   "source": [
    "# Exploratory Data Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b100c",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84514cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:  \n",
    "    df_stats = df.apply(lambda x: x.fillna('').str.split().apply(len))   \n",
    "    summary_stats = df_stats.describe()  \n",
    "    for col in df_stats.columns:  \n",
    "        summary_stats.loc['empty', col] = df_stats[col].value_counts(sort = False).get(0, 0)  \n",
    "        summary_stats.loc['not_empty', col] = summary_stats.loc['count', col] - summary_stats.loc['empty', col]  \n",
    "    # Sort the columns by the 'not_empty' row in descending order  \n",
    "    df_stats = df_stats[summary_stats.loc['not_empty'].sort_values(ascending=False).index]  \n",
    "    display(summary_stats)  \n",
    "              \n",
    "    # Create a box and whisker plot  \n",
    "    plt.boxplot(df_stats,  \n",
    "               flierprops={'marker': '.'},  \n",
    "               notch = True)  \n",
    "    plt.xticks(rotation=45)  \n",
    "    plt.xticks(range(1, len(df_stats.columns) + 1), df_stats.columns)  \n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c2575",
   "metadata": {},
   "source": [
    "## Unique words and occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "if EDA:\n",
    "    words_dict = {}\n",
    "\n",
    "    for column in df:\n",
    "        for index, row in df.iterrows():\n",
    "                words_dict = ipw.text.add_word_count_to_dict(row[column], words_dict)\n",
    "\n",
    "    total_words = sum(words_dict.values())\n",
    "    print(f'Total words: {total_words}')\n",
    "    print(f'Unique words: {len(words_dict)}')\n",
    "    print(f'Average occurance: {total_words / len(words_dict):.2f}')\n",
    "    \n",
    "    ipw.plots.wordcloud(words_dict).show()\n",
    "    ipw.plots.histogram_wordfreq(words_dict).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7035edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description column\n",
    "if EDA:\n",
    "    words_dict = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        words_dict = ipw.text.add_word_count_to_dict(row['description'], words_dict)\n",
    "\n",
    "    total_words = sum(words_dict.values())\n",
    "    print(f'Total words: {total_words}')\n",
    "    print(f'Unique words: {len(words_dict)}')\n",
    "    print(f'Average occurance: {total_words / len(words_dict):.2f}')\n",
    "    \n",
    "    ipw.plots.wordcloud(words_dict).show()\n",
    "    ipw.plots.histogram_wordfreq(words_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d07e1",
   "metadata": {},
   "source": [
    "# Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cedbaa",
   "metadata": {},
   "source": [
    "## Nouns in description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_1 = df['description'].apply(ipw.text.filter).to_frame('text')\n",
    "df_model_1['description'] = df['description']\n",
    "\n",
    "obs_dict = {}\n",
    "\n",
    "for index, row in df_model_1.iterrows():\n",
    "    obs_dict = ipw.text.add_word_count_to_dict(row['text'], obs_dict)\n",
    "     \n",
    "total_words = sum(obs_dict.values())\n",
    "print(f'Total words for model: {total_words}')\n",
    "print(f'Unique words: {len(obs_dict)}')\n",
    "print(f'Average occurance: {total_words / len(obs_dict):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw.plots.wordcloud(obs_dict, '').show()\n",
    "ipw.plots.histogram_wordfreq(obs_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cb945",
   "metadata": {},
   "source": [
    "## spaCy vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nl_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load natural language model for dutch\n",
    "nlp = nl_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of series objects representing the columns of the new DataFrame  \n",
    "vector_names = [f\"V{i}\" for i in range(nlp.vocab.vectors_length)]  \n",
    "column_list = [] \n",
    "\n",
    "for name in vector_names:\n",
    "     column_list.append(pd.Series(name=name, index=df.index, dtype=float))  \n",
    "  \n",
    "# Loop over the strings in the original DataFrame and add their spaCy vectors to the column Series objects  \n",
    "for i, text in enumerate(df_model_1['text']):  \n",
    "    doc = nlp(text)  \n",
    "    for j, value in enumerate(doc.vector):  \n",
    "        column_list[j][i] = value  \n",
    "  \n",
    "# Concatenate the column Series objects to create the new DataFrame  \n",
    "df_vector_1 = pd.concat(column_list, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad3fa5",
   "metadata": {},
   "source": [
    "## Remove samples that have empty vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the norm of each row using np.linalg.norm()  \n",
    "norms = df_vector_1.apply(lambda row: np.linalg.norm(row), axis=1)  \n",
    "\n",
    "print(f'Number of samples before selection: {len(df)}')\n",
    "# Filter out the rows where the norm is zero  \n",
    "df_vector_1 = df_vector_1[norms != 0]  \n",
    "\n",
    "print(f'Number of samples for clustering: {len(df_vector_1)}')\n",
    "\n",
    "# also remove from the original dataframe with text\n",
    "df_model_1 = df_model_1[df_model_1.index.isin(df_vector_1.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba4a1e",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de07756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    correlation = df_vector_1.corr()\n",
    "    correlation = np.triu(correlation, k=1)  # Keep only the upper triangle of the correlation matrix  \n",
    "\n",
    "    sns.heatmap(abs(correlation), center = 0, cmap=\"RdBu\", vmax = 1.0, vmin = 0.0)\n",
    "    print(f'Max correlation: {correlation.max() :.3f}')  \n",
    "    print(f'Min correlation: {correlation.min() :.3f}')  \n",
    "    print(f'Mean absolute correlation: {(abs(correlation)).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942ecc5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09486e9f",
   "metadata": {},
   "source": [
    "## Set Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c633c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to be able to get repeatable results\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71f763",
   "metadata": {},
   "source": [
    "## Cosine Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = ipw.models.distance_matrix(df_vector_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425573e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS \n",
    "cmap = 'viridis' #standard\n",
    "#cmap = 'hot' #oranges and reds\n",
    "mds = MDS(n_components=4, dissimilarity='precomputed', normalized_stress = 'auto', random_state=RANDOM_STATE, )  \n",
    "coords = mds.fit_transform(dist_matrix)  \n",
    "  \n",
    "# Create a scatterplot of the coordinates  \n",
    "fig = plt.figure()  \n",
    "ax = fig.add_subplot(111, projection='3d')  \n",
    "ax.scatter(coords[:, 0], coords[:, 1], coords[:, 2], c= coords[:, 3], cmap = cmap)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=3, dissimilarity='precomputed', normalized_stress = 'auto', random_state=RANDOM_STATE, )  \n",
    "coords = mds.fit_transform(dist_matrix)  \n",
    "  \n",
    "# Create a scatterplot of the coordinates  \n",
    "fig = plt.figure()  \n",
    "plt.scatter(coords[:, 0], coords[:, 1], c= coords[:, 2], cmap = cmap)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7e95",
   "metadata": {},
   "source": [
    "## Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c63ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ipw.enums.Model.AFFINITY_PROPAGATION\n",
    "dampings = np.arange(0.5, 1.0, 0.05) \n",
    "af_results, af_labels, af_centers = ipw.models.affinity_propagation(dist_matrix, dampings, random_state = RANDOM_STATE)\n",
    "df_model_1[model.col(1)] = af_labels\n",
    "ipw.models.output(dist_matrix, af_results, af_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(af_labels)\n",
    "    ipw.plots.silhouette(dist_matrix, af_labels)\n",
    "    ipw.plots.model_wordclouds(df_model_1, model, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52424a",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ipw.enums.Model.AGGLOMERATIVE_CLUSTERING\n",
    "n_clusters = range(2, 20)\n",
    "ac_results, ac_labels = ipw.models.agglomerative_clustering(dist_matrix, n_clusters, RANDOM_STATE)\n",
    "df_model_1[model.col(1)] = ac_labels\n",
    "ipw.models.output(dist_matrix, ac_results, ac_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(ac_labels)\n",
    "    ipw.plots.silhouette(dist_matrix, ac_labels)\n",
    "    ipw.plots.model_wordclouds(df_model_1, model, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b0df",
   "metadata": {},
   "source": [
    "# DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5468c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ipw.enums.Model.DBSCAN\n",
    "eps_arr = np.arange(0.01, 0.2, 0.01) \n",
    "min_samples = range(2, 10)\n",
    "db_results, db_labels = ipw.models.dbscan(dist_matrix, eps_arr, min_samples)\n",
    "df_model_1[model.col(1)] = db_labels\n",
    "ipw.models.output(dist_matrix, db_results, db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(db_labels)\n",
    "    ipw.plots.silhouette(dist_matrix, db_labels)\n",
    "    ipw.plots.model_wordclouds(df_model_1, model, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1840c5",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ipw.enums.Model.SPECTRAL_CLUSTERING\n",
    "n_clusters = range(2, 10)\n",
    "sc_results, sc_labels = ipw.models.spectral_clustering(dist_matrix, n_clusters, RANDOM_STATE)\n",
    "df_model_1[model.col(1)] = sc_labels\n",
    "ipw.models.output(dist_matrix, sc_results, sc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(sc_labels)\n",
    "    ipw.plots.silhouette(dist_matrix, sc_labels)\n",
    "    ipw.plots.model_wordclouds(df_model_1, model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93275f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = ipw.text.description_per_label(df_model_1, model, 1)\n",
    "print(ipw.text.summary(descriptions[0], 5))\n",
    "print('-----')\n",
    "print(ipw.text.summary(descriptions[1], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76599001",
   "metadata": {},
   "source": [
    "# Second Round on column description with Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c144fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ipw.enums.Model.SPECTRAL_CLUSTERING\n",
    "n_clusters = range(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a96136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_2a = df_model_1[df_model_1[model1.col(1)] == 0].copy()\n",
    "df_vector_2a = df_vector_1[df_vector_1.index.isin(df_model_2a.index)]\n",
    "dist_matrix_2a = ipw.distance_matrix(df_vector_2a)\n",
    "\n",
    "sc_results2a, sc_labels2a = ipw.models.spectral_clustering(dist_matrix_2a, n_clusters, RANDOM_STATE)\n",
    "df_model_2a[model1.col(2)] = sc_labels2a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_2b = df_model_1[df_model_1[model1.col(1)] == 1].copy()\n",
    "df_vector_2b = df_vector_1[df_vector_1.index.isin(df_model_2b.index)]\n",
    "dist_matrix_2b = ipw.distance_matrix(df_vector_2b)\n",
    "\n",
    "sc_results2b, sc_labels2b = ipw.models.spectral_clustering(dist_matrix_2b, n_clusters, RANDOM_STATE)\n",
    "df_model_2b[model1.col(2)] = sc_labels2b + 2 + sc_labels2a.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([df_model_2a, df_model_2b], axis = 0)[[model1.col(2)]]\n",
    "df_model_2 = df_model_1.join(merged, how = 'left')\n",
    "df_model_2[df_model_2[model1.col(2)] == None] = 0\n",
    "labels_2 = df_model_2[model1.col(2)]\n",
    "\n",
    "n = labels_2.max()\n",
    "print('Cluster size per cluster')\n",
    "for i in range(min(labels_2), max(labels_2+1)):\n",
    "    print(f'Cluster {i}: {sum(labels_2 == i)}')\n",
    "print('----')\n",
    "sc = metrics.silhouette_score(dist_matrix, labels_2, metric=\"precomputed\")\n",
    "vrc = metrics.calinski_harabasz_score(dist_matrix, labels_2)\n",
    "dbi = metrics.davies_bouldin_score(dist_matrix, labels_2)\n",
    "\n",
    "print(f'Number of clusters: {labels_2.max() - labels_2.min() + 1}')\n",
    "print(f'Silhouette Coefficient: {sc:0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {vrc:0.3f}')\n",
    "print(f'Davies-Bouldin Index: {dbi:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(labels_2)\n",
    "    ipw.plots.silhouette(dist_matrix, labels_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74313b",
   "metadata": {},
   "source": [
    "# Second Round on column solution with Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f077b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_3 = df['solution'].apply(ipw.text.filter).to_frame('text')\n",
    "df_model_3['solution'] = df['solution']\n",
    "df_model_3 = df_model_3[df_model_3.index.isin(df_model_1.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of series objects representing the columns of the new DataFrame  \n",
    "vector_names = [f\"V{i}\" for i in range(nlp.vocab.vectors_length)]  \n",
    "column_list = [] \n",
    "\n",
    "for name in vector_names:\n",
    "     column_list.append(pd.Series(name=name, index=df_model_3.index, dtype=float))  \n",
    "  \n",
    "# Loop over the strings in the original DataFrame and add their spaCy vectors to the column Series objects  \n",
    "for i, text in enumerate(df_model_3['text']):  \n",
    "    doc = nlp(text)  \n",
    "    for j, value in enumerate(doc.vector):  \n",
    "        column_list[j][i] = value  \n",
    "  \n",
    "# Concatenate the column Series objects to create the new DataFrame  \n",
    "df_vector_3 = pd.concat(column_list, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83febed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the norm of each row using np.linalg.norm()  \n",
    "norms = df_vector_3.apply(lambda row: np.linalg.norm(row), axis=1)  \n",
    "\n",
    "print(f'Number of samples before selection: {len(df)}')\n",
    "# Filter out the rows where the norm is zero  \n",
    "df_vector_3 = df_vector_3[norms != 0]  \n",
    "\n",
    "print(f'Number of samples for clustering: {len(df_vector_3)}')\n",
    "\n",
    "# also remove from the original dataframe with text\n",
    "df_model_3 = df_model_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_3a = df_vector_3[df_vector_3.index.isin(df_model_2a.index)].copy()\n",
    "df_model_3a = df_model_3[df_model_3.index.isin(df_vector_3a.index)].copy()\n",
    "dist_matrix_3a = ipw.distance_matrix(df_vector_3a)\n",
    "\n",
    "sc_results3a, sc_labels3a = ipw.models.spectral_clustering(dist_matrix_3a, n_clusters, RANDOM_STATE)\n",
    "df_model_3a[model1.col(3)] = (sc_labels3a + 2)\n",
    "ipw.models.output(dist_matrix_3a, sc_results3a, sc_labels3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_3b = df_vector_3[df_vector_3.index.isin(df_model_2b.index)].copy()\n",
    "df_model_3b = df_model_3[df_model_3.index.isin(df_vector_3b.index)].copy()\n",
    "dist_matrix_3b = ipw.distance_matrix(df_vector_3b)\n",
    "\n",
    "sc_results3b, sc_labels3b = ipw.models.spectral_clustering(dist_matrix_3b, n_clusters, RANDOM_STATE)\n",
    "df_model_3b[model1.col(3)] = (sc_labels3a.max() + 3 + sc_labels3b)\n",
    "ipw.models.output(dist_matrix_3b, sc_results3b, sc_labels3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbfaf94",
   "metadata": {},
   "source": [
    "## Output with distance matrix based on description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([df_model_3a, df_model_3b], axis = 0)[[model1.col(3)]]\n",
    "df_model_3 = df_model_3.join(merged, how = 'left')\n",
    "df_model_3[model1.col(3)] = df_model_3[model1.col(3)].fillna(df_model_3[model1.col(1)]).astype(int)\n",
    "labels_3 = df_model_3[model1.col(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604368e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_3 = df_model_3[model1.col(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a92c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(labels_3.max())\n",
    "print('Cluster size per cluster')\n",
    "for i in range(n+1):\n",
    "    print(f'Cluster {i}: {sum(labels_3 == i)}')\n",
    "print('----')\n",
    "sc = metrics.silhouette_score(dist_matrix, labels_3, metric=\"precomputed\")\n",
    "vrc = metrics.calinski_harabasz_score(dist_matrix, labels_3)\n",
    "dbi = metrics.davies_bouldin_score(dist_matrix, labels_3)\n",
    "\n",
    "print(f'Number of clusters: {labels_3.max() - labels_3.min() + 1}')\n",
    "print(f'Silhouette Coefficient: {sc:0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {vrc:0.3f}')\n",
    "print(f'Davies-Bouldin Index: {dbi:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(labels_3)\n",
    "    ipw.plots.silhouette(dist_matrix, labels_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c417970",
   "metadata": {},
   "source": [
    "## Output with distance matrix based on solution column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_model_3[df_model_3.index.isin(df_vector_3.index)].copy()\n",
    "labels_4 = df_output[model1.col(3)]\n",
    "distance_sol = ipw.models.distance_matrix(df_vector_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5084124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(labels_4.max())\n",
    "print('Cluster size per cluster')\n",
    "for i in range(n+1):\n",
    "    print(f'Cluster {i}: {sum(labels_4 == i)}')\n",
    "print('----')\n",
    "sc = metrics.silhouette_score(distance_sol, labels_4, metric=\"precomputed\")\n",
    "vrc = metrics.calinski_harabasz_score(distance_sol, labels_4)\n",
    "dbi = metrics.davies_bouldin_score(distance_sol, labels_4)\n",
    "\n",
    "print(f'Number of clusters: {labels_3.max() - labels_3.min() + 1}')\n",
    "print(f'Silhouette Coefficient: {sc:0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {vrc:0.3f}')\n",
    "print(f'Davies-Bouldin Index: {dbi:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_MODEL_PLOTS:\n",
    "    ipw.plots.bar_labels(labels_4)\n",
    "    ipw.plots.silhouette(distance_sol, labels_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "  \n",
    "def histogram_labels(labels):  \n",
    "    label_dict = {}  \n",
    "          \n",
    "    for label in labels:  \n",
    "        if label in label_dict:  \n",
    "            label_dict[label] += 1  \n",
    "        else:  \n",
    "            label_dict[label] = 1  \n",
    "  \n",
    "    sorted_labels = sorted(label_dict.items())  \n",
    "  \n",
    "    plt.bar([f'cluster {key}' for key, value in sorted_labels], [value for key, value in sorted_labels])  \n",
    "  \n",
    "    # add labels and title to the chart    \n",
    "    plt.xlabel(\"Cluster size\")    \n",
    "    plt.ylabel(\"Frequency\")    \n",
    "    plt.title(\"Cluster size Frequency Histogram\")    \n",
    "    plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec44492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec042ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
