{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import pyarrow.dataset as ds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nl_core_news_lg\n",
    "from typing import Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = 'log.txt', level = logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8b617",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1defc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = Path(r'C:/Git/HonoursProject/ipw-classifier/ipw_classifier/data/')\n",
    "PARQUET_SUFFIX = '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse(table_name: str) -> pd.DataFrame:\n",
    "    parquet_path = PATH_DATA / f\"{table_name}{PARQUET_SUFFIX}\"\n",
    "    table = ds.dataset(parquet_path).to_table()\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    logging.info(f'Number of records in table {table_name}: {len(df)}')\n",
    "\n",
    "    if 'case_id' in df.columns:\n",
    "        if not df['case_id'].is_unique:\n",
    "            raise ValueError(f'Duplicate values found in \"case_id\" column in table {table_name}.')\n",
    "        \n",
    "        df.rename(columns = {'id': f'{table_name}_id'}, inplace = True)\n",
    "    \n",
    "    columns_to_drop = set([\n",
    "        'author',\n",
    "        'rights', \n",
    "        'created',\n",
    "        'updated',\n",
    "        'deleted',\n",
    "        'owners',\n",
    "        'source',\n",
    "        'closed'\n",
    "        ])\n",
    "    columns = columns_to_drop.intersection(set(df.columns))  \n",
    "    \n",
    "    for col_name in columns:\n",
    "        df.drop(col_name, axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab049da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    case = _parse('case')\n",
    "    situation = _parse('situation')\n",
    "    plan = _parse('plan')\n",
    "    \n",
    "    df_sit_pln = plan.merge(situation, left_on = 'case_id', right_on = 'case_id', how = 'outer', suffixes = ('_pln', '_sit'))\n",
    "    df = df_sit_pln.merge(case, left_on = 'case_id', right_on = 'id', how = 'left')\n",
    "    logging.info(f'Number of records in combined table: {len(df)}')\n",
    "    df = df[df['status'] == 'closed']\n",
    "    logging.info(f'Number of closed records in combined table: {len(df)}')\n",
    "    \n",
    "    df.drop('case_id', axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ecf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    'plan_id',\n",
    "    'situation_id',\n",
    "    'title',\n",
    "    'status',\n",
    "    'collection_id',\n",
    "    'author_id'\n",
    "]\n",
    "\n",
    "df = df_in.drop(to_drop, axis = 1)\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd9bb8",
   "metadata": {},
   "source": [
    "## Clean the text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a41e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string:str) -> str:\n",
    "    returnvalue = ''\n",
    "    if string is not None and not isinstance(string, float):\n",
    "        \n",
    "        # parse html\n",
    "        soup = BeautifulSoup(string, 'html.parser')\n",
    "        returnvalue = soup.getText()\n",
    "        \n",
    "        # remove '\\n'\n",
    "        returnvalue = returnvalue.replace('\\\\n', '')\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:  \n",
    "    df[column] = df[column].apply(clean_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2e8d",
   "metadata": {},
   "source": [
    "# Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84514cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the length of each string field in words per record  \n",
    "df_stats = df.apply(lambda x: x.fillna('').str.split().apply(len)) \n",
    "summary_stats = df_stats.describe()\n",
    "for col in df_stats.columns:\n",
    "    summary_stats.loc['empty', col] = df_stats[col].value_counts(sort = False).get(0, 0)\n",
    "    summary_stats.loc['not_empty', col] = summary_stats.loc['count', col] - summary_stats.loc['empty', col]\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ffca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box and whisker plot\n",
    "plt.boxplot(df_stats)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(range(1, len(df_stats.columns) + 1), df_stats.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c2575",
   "metadata": {},
   "source": [
    "## Unique words and occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string:str, returnvalue: Dict[str, int]) -> Dict[str, int]:\n",
    "    words = string.split()\n",
    "    for word in words:\n",
    "        key = re.sub(r'[^a-z]', '', word.lower())\n",
    "        if key in returnvalue:\n",
    "            returnvalue[key] += 1\n",
    "        else:\n",
    "            returnvalue[key] = 1\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33961c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(dict):\n",
    "#calculate the optimal distribution of bins according to Freedman-Draconis\n",
    "    data = list(dict.values())\n",
    "\n",
    "    # create bins for the histogram  \n",
    "    #bins = np.exp(bins_sturge(np.log(data)))\n",
    "    log_data = np.log(data)\n",
    "    \n",
    "    iqr = np.percentile(log_data, 75) - np.percentile(log_data, 25)\n",
    "    bin_width = (2 * iqr) / (len(log_data) ** (1 / 3))\n",
    "    log_bins = np.arange(min(log_data), max(log_data), bin_width)\n",
    "    bins = np.exp(log_bins)\n",
    "\n",
    "    # create the histogram  \n",
    "    alpha = 1\n",
    "    plt.hist(data, bins=bins, align='left', color = 'blue', alpha = alpha)\n",
    "\n",
    "    # add labels and title to the chart  \n",
    "    plt.xlabel(\"Frequency (log scale)\")  \n",
    "    plt.ylabel(\"Occurrences (log scale)\")  \n",
    "    plt.title(\"Word Frequency Histogram\")  \n",
    "\n",
    "    # set the axis to a logarithmic scale  \n",
    "    plt.xscale('log')  \n",
    "    plt.yscale('log')  \n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_from_dict(dict: Dict[str, int], w:int = 12, h:int = 8)-> WordCloud:\n",
    "    if not dict:  \n",
    "        dict = {\"NO WORDS\": 1} \n",
    "    \n",
    "    return WordCloud(width = w * 100, \n",
    "                     height= h * 100,\n",
    "                     background_color=\"white\", \n",
    "                     prefer_horizontal=0.8,  \n",
    "                     min_font_size=10, \n",
    "                     max_font_size=400).generate_from_frequencies(dict)  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud_from_dict(dict: Dict[str, int], w:int = 12, h:int = 8)-> plt:\n",
    "    wordcloud = wordcloud_from_dict(dict, w, h)\n",
    "    \n",
    "# Display the generated image  \n",
    "    plt.figure(figsize=(w, h))  \n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")  \n",
    "    plt.axis(\"off\")  \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "\n",
    "for column in df:\n",
    "    for index, row in df.iterrows():\n",
    "            words_dict = word_count(row[column], words_dict)\n",
    "     \n",
    "total_words = sum(words_dict.values())\n",
    "print(f'Total words: {total_words}')\n",
    "print(f'Unique words: {len(words_dict)}')\n",
    "print(f'Average occurance: {total_words / len(words_dict):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7035edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud_from_dict(words_dict, 12, 8).show()\n",
    "histogram(words_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cedbaa",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load natural language model for dutch\n",
    "nlp = nl_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d07e1",
   "metadata": {},
   "source": [
    "## Transform the data to only consider nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ee49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nouns(string: str) -> str:\n",
    "    doc = nlp(string)\n",
    "    nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "    noun_str = [noun.text for noun in nouns]\n",
    "    returnvalue = ' '.join(noun_str)\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539761b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for column in df.columns:  \n",
    "    nouns.append(df[column].apply(filter_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(nouns[0])\n",
    "strings = []\n",
    "for i in range (n):\n",
    "    string = ' '.join([row[i] for row in nouns])\n",
    "    strings.append(string)\n",
    "df['obs'] = strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d01d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_dict = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    nouns_dict = word_count(row['obs'], nouns_dict)\n",
    "     \n",
    "total_words = sum(nouns_dict.values())\n",
    "print(f'Total nouns: {total_words}')\n",
    "print(f'Unique nouns: {len(nouns_dict)}')\n",
    "print(f'Average occurance: {total_words / len(nouns_dict):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud_from_dict(nouns_dict, 12, 8).show()\n",
    "histogram(nouns_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cb945",
   "metadata": {},
   "source": [
    "# Transform the column we want to consider to a SpaCy vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of series objects representing the columns of the new DataFrame  \n",
    "vector = nlp('tekst for vector').vector # create an arbitrary vector to be certain that we have the correct length\n",
    "vector_names = [f\"V{i}\" for i in range(len(vector))]  \n",
    "column_list = [] \n",
    "\n",
    "for name in vector_names:\n",
    "     column_list.append(pd.Series(name=name, index=df.index, dtype=float))  \n",
    "  \n",
    "# Loop over the strings in the original DataFrame and add their spaCy vectors to the column Series objects  \n",
    "for i, text in enumerate(df['obs']):  \n",
    "    doc = nlp(text)  \n",
    "    for j, value in enumerate(doc.vector):  \n",
    "        column_list[j][i] = value  \n",
    "  \n",
    "# Concatenate the column Series objects to create the new DataFrame  \n",
    "df_vec = pd.concat(column_list, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfdd46",
   "metadata": {},
   "source": [
    "## Remove samples that have empty vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the norm of each row using np.linalg.norm()  \n",
    "norms = df_vec.apply(lambda row: np.linalg.norm(row), axis=1)  \n",
    "  \n",
    "# Filter out the rows where the norm is zero  \n",
    "df_vec = df_vec[norms != 0]  \n",
    "\n",
    "print(f'Number of samples for clustering: {len(df_vec)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09486e9f",
   "metadata": {},
   "source": [
    "## Set Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = None\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba4a1e",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore the correlations in our data set \n",
    "plt.figure(figsize=(10,10))\n",
    "correlation = df_vec.corr()\n",
    "# Keep only the upper triangle of the correlation matrix  \n",
    "correlation = np.triu(correlation, k=1)  \n",
    "\n",
    "sns.heatmap(abs(correlation), center = 0, cmap=\"RdBu\", vmax = 1.0, vmin = 0.0)\n",
    "\n",
    "# The correlation measure used here is Pearson’s correlation. \n",
    "# In our case the lighter the square the stronger the correlation between two variables.\n",
    "\n",
    "# Print the maximum and minimum correlations  \n",
    "max_corr = correlation.max()  \n",
    "min_corr = correlation.min()\n",
    "avg_corr = abs(correlation).mean()\n",
    "print(f'Max correlation: {max_corr:.3f}')  \n",
    "print(f'Min correlation: {min_corr:.3f}')  \n",
    "print(f'Mean absolute correlation: {avg_corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7e95",
   "metadata": {},
   "source": [
    "# Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_vec)\n",
    "\n",
    "aff_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        vector_i = df_vec.iloc[i]\n",
    "        vector_j = df_vec.iloc[j]\n",
    "        \n",
    "        similarity = np.dot(vector_i, vector_j) / (np.linalg.norm(vector_i) * np.linalg.norm(vector_j))\n",
    "        similarity = max(-1.0, min(1.0, similarity)) # to remove corner cases from rounding\n",
    "        # to create a distance that is 0 for equal cases (similarity = 1) and 2 for cases that are very far apart\n",
    "        distance = 1 - similarity\n",
    "        aff_matrix[i][j] = aff_matrix[j][i] = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84feea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "af1 = AffinityPropagation(random_state = RANDOM_STATE, verbose = False, max_iter = 500).fit(df_vec)\n",
    "cluster_centers_indices1 = af1.cluster_centers_indices_\n",
    "labels1 = af1.labels_\n",
    "n_clusters1 = len(cluster_centers_indices1)\n",
    "  \n",
    "print(f'Estimated number of clusters: {n_clusters1}')\n",
    "print(f'Silhouette Coefficient: {metrics.silhouette_score(df_vec, labels1, metric=\"sqeuclidean\"):0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {metrics.calinski_harabasz_score(df_vec, labels1):0.3f}')\n",
    "print(f'Davies-Bouldin Index: {metrics.davies_bouldin_score(df_vec, labels1):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3569f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "af = AffinityPropagation(\n",
    "    random_state = RANDOM_STATE, \n",
    "    verbose = False,\n",
    "    max_iter = 500, \n",
    "    affinity = 'precomputed').fit(aff_matrix)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "  \n",
    "print(f'Estimated number of clusters: {n_clusters}')\n",
    "print(f'Silhouette Coefficient: {metrics.silhouette_score(aff_matrix, labels, metric=\"sqeuclidean\"):0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {metrics.calinski_harabasz_score(aff_matrix, labels):0.3f}')\n",
    "print(f'Davies-Bouldin Index: {metrics.davies_bouldin_score(aff_matrix, labels):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dicts = {}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    words = df.loc[df_vec.index[i]]['obs']\n",
    "    \n",
    "    if label in label_dicts:\n",
    "        label_dicts[label] = word_count(words, label_dicts[label])\n",
    "    else:\n",
    "        dict = {}\n",
    "        label_dicts[label] = word_count(words, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ff9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordclouds = [wordcloud_from_dict(value, 7, 7) for value in label_dicts.values()]\n",
    "\n",
    "# Create a grid of subplots, ncols wide\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(wordclouds) / ncols))\n",
    "nplots = ncols * nrows\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20))  \n",
    "keys = list(label_dicts.keys())\n",
    "    \n",
    "# Plot each individual wordcloud in a separate subplot  \n",
    "for i in range(nplots): \n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "    if nrows == 1:\n",
    "        if i < len(wordclouds):\n",
    "            axs[col].imshow(wordclouds[i].to_array(), interpolation='bilinear')  \n",
    "            axs[col].set_title(f'Cluster {keys[i]}', pad = 15)\n",
    "        axs[col].axis('off')  \n",
    "    else:\n",
    "        if i < len(wordclouds):\n",
    "            axs[row, col].imshow(wordclouds[i].to_array(), interpolation='bilinear')  \n",
    "            axs[row, col].set_title(f'Cluster {keys[i]}')\n",
    "        axs[row, col].axis('off')  \n",
    "\n",
    "  \n",
    "# Show the grid of subplots  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3531029",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for label in labels:\n",
    "    if label in freq:\n",
    "        freq[label] += 1\n",
    "    else:\n",
    "        freq[label] = 1\n",
    "        \n",
    "data = freq.values()\n",
    "plt.hist(data)\n",
    "\n",
    "# add labels and title to the chart  \n",
    "plt.xlabel(\"Cluster size\")  \n",
    "plt.ylabel(\"Frequency\")  \n",
    "plt.title(\"Cluster size Frequency Histogram\")  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = metrics.silhouette_samples(df_vec, labels)\n",
    "min_sil = math.floor(min(sample_silhouette_values) / 0.2) * 0.2\n",
    "max_sil = max(sample_silhouette_values)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "ax.set_xlim([min_sil, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette plots of individual clusters,\n",
    "# to demarcate them clearly.    \n",
    "ax.set_ylim([0, len(df_vec) + (n_clusters + 1) * 10])\n",
    "\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "# This gives a perspective into the density and separation of the formed\n",
    "# clusters\n",
    "silhouette_avg = metrics.silhouette_score(df_vec, labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax.text(min_sil, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    xticks = np.arange(min_sil, 1.1, 0.2)  \n",
    "    ax.set_xticks(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2c435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
