{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa71dd5",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343435",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = r'C:/Git/HonoursProject/ipw-classifier/ipw_classifier/src'\n",
    "DATA_PATH = r'C:/Git/HonoursProject/ipw-classifier/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add source python files to project\n",
    "import sys\n",
    "sys.path.insert(0, CODE_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nl_core_news_lg\n",
    "from typing import Dict, List\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.cluster import AffinityPropagation, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn import metrics\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MaxNLocator  \n",
    "from spacy.lang.nl.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = 'log.txt', level = logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8b617",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parse_input\n",
    "df = parse_input.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e944380",
   "metadata": {},
   "source": [
    "## Nr of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d466b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total records: {len(df)}')\n",
    "print('Records per status:')\n",
    "print(df['status'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select closed records\n",
    "df = df[df['status'] == 'closed']\n",
    "df.drop('status', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd9bb8",
   "metadata": {},
   "source": [
    "## Clean the text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_input.clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2e8d",
   "metadata": {},
   "source": [
    "# Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84514cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the length of each string field in words per record  \n",
    "df_stats = df.apply(lambda x: x.fillna('').str.split().apply(len)) \n",
    "summary_stats = df_stats.describe()\n",
    "for col in df_stats.columns:\n",
    "    summary_stats.loc['empty', col] = df_stats[col].value_counts(sort = False).get(0, 0)\n",
    "    summary_stats.loc['not_empty', col] = summary_stats.loc['count', col] - summary_stats.loc['empty', col]\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ffca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box and whisker plot\n",
    "plt.boxplot(df_stats)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(range(1, len(df_stats.columns) + 1), df_stats.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c2575",
   "metadata": {},
   "source": [
    "## Unique words and occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string:str, returnvalue: Dict[str, int]) -> Dict[str, int]:\n",
    "    words = string.split()\n",
    "    for word in words:\n",
    "        key = re.sub(r'[^a-z]', '', word.lower())\n",
    "        if key in returnvalue:\n",
    "            returnvalue[key] += 1\n",
    "        else:\n",
    "            returnvalue[key] = 1\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33961c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(dict):\n",
    "#calculate the optimal distribution of bins according to Freedman-Draconis\n",
    "    data = list(dict.values())\n",
    "\n",
    "    # create bins for the histogram  \n",
    "    #bins = np.exp(bins_sturge(np.log(data)))\n",
    "    log_data = np.log(data)\n",
    "    \n",
    "    iqr = np.percentile(log_data, 75) - np.percentile(log_data, 25)\n",
    "    bin_width = (2 * iqr) / (len(log_data) ** (1 / 3))\n",
    "    log_bins = np.arange(min(log_data), max(log_data), bin_width)\n",
    "    bins = np.exp(log_bins)\n",
    "\n",
    "    # create the histogram  \n",
    "    alpha = 1\n",
    "    plt.hist(data, bins=bins, align='left', color = 'blue', alpha = alpha)\n",
    "\n",
    "    # add labels and title to the chart  \n",
    "    plt.xlabel(\"Frequency (log scale)\")  \n",
    "    plt.ylabel(\"Occurrences (log scale)\")  \n",
    "    plt.title(\"Word Frequency Histogram\")  \n",
    "\n",
    "    # set the axis to a logarithmic scale  \n",
    "    plt.xscale('log')  \n",
    "    plt.yscale('log')  \n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_from_dict(dict: Dict[str, int], w:int = 12, h:int = 8)-> WordCloud:\n",
    "    if not dict:  \n",
    "        dict = {\"NO WORDS\": 1} \n",
    "    \n",
    "    return WordCloud(width = w * 100, \n",
    "                     height= h * 100,\n",
    "                     background_color=\"white\", \n",
    "                     prefer_horizontal=0.8,  \n",
    "                     min_font_size=10, \n",
    "                     max_font_size=400).generate_from_frequencies(dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud_from_dict(dict: Dict[str, int], w:int = 12, h:int = 8)-> plt:\n",
    "    wordcloud = wordcloud_from_dict(dict, w, h)\n",
    "    \n",
    "# Display the generated image  \n",
    "    plt.figure(figsize=(w, h))  \n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")  \n",
    "    plt.axis(\"off\")  \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "\n",
    "for column in df:\n",
    "    for index, row in df.iterrows():\n",
    "            words_dict = word_count(row[column], words_dict)\n",
    "     \n",
    "total_words = sum(words_dict.values())\n",
    "print(f'Total words: {total_words}')\n",
    "print(f'Unique words: {len(words_dict)}')\n",
    "print(f'Average occurance: {total_words / len(words_dict):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7035edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud_from_dict(words_dict, 12, 8).show()\n",
    "histogram(words_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cedbaa",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load natural language model for dutch\n",
    "nlp = nl_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f17da",
   "metadata": {},
   "source": [
    "# Text summary\n",
    "Source: https://www.kaggle.com/code/itsmohammadshahid/nlp-text-summarizer-using-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f61bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textSummarizer(text, sentences):\n",
    "    # pass the text into the nlp function\n",
    "    doc= nlp(text)\n",
    "    \n",
    "    ## The score of each word is kept in a frequency table\n",
    "    tokens=[token.text for token in doc]\n",
    "    freq_of_word=dict()\n",
    "    \n",
    "    # Text cleaning and vectorization \n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in freq_of_word.keys():\n",
    "                    freq_of_word[word.text] = 1\n",
    "                else:\n",
    "                    freq_of_word[word.text] += 1\n",
    "\n",
    "    if not bool(freq_of_word): return ''\n",
    "    # Maximum frequency of word\n",
    "    max_freq=max(freq_of_word.values())\n",
    "    \n",
    "    # Normalization of word frequency\n",
    "    for word in freq_of_word.keys():\n",
    "        freq_of_word[word]=freq_of_word[word]/max_freq\n",
    "        \n",
    "    # In this part, each sentence is weighed based on how often it contains the token.\n",
    "    sent_tokens= [sent for sent in doc.sents]\n",
    "    sent_scores = dict()\n",
    "    for sent in sent_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in freq_of_word.keys():\n",
    "                if sent not in sent_scores.keys():                            \n",
    "                    sent_scores[sent]=freq_of_word[word.text.lower()]\n",
    "                else:\n",
    "                    sent_scores[sent]+=freq_of_word[word.text.lower()]\n",
    "    \n",
    "    \n",
    "    # Summary for the sentences with maximum score. Here, each sentence in the list is of spacy.span type\n",
    "    summary = nlargest(n = sentences, iterable = sent_scores, key = sent_scores.get)\n",
    "    \n",
    "    # Prepare for final summary\n",
    "    final_summary=[word.text for word in summary]\n",
    "    \n",
    "    #convert to a string\n",
    "    summary=\" \".join(final_summary)\n",
    "    \n",
    "    # Return final summary\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a32f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['description'].apply(textSummarizer, sentences = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d07e1",
   "metadata": {},
   "source": [
    "## Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ee49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(string: str) -> str:\n",
    "    doc = nlp(string)\n",
    "    tokens = [token for token in doc if token.pos_ == 'NOUN']\n",
    "    token_str = [token.text for token in tokens if len(token.text) > 2]\n",
    "    returnvalue = ' '.join(token_str)\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['obs'] = df['summary'].apply(filter_tokens)\n",
    "df['obs'] = df['description'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d01d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    obs_dict = word_count(row['obs'], obs_dict)\n",
    "     \n",
    "total_words = sum(obs_dict.values())\n",
    "print(f'Total nouns: {total_words}')\n",
    "print(f'Unique nouns: {len(obs_dict)}')\n",
    "print(f'Average occurance: {total_words / len(obs_dict):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud_from_dict(obs_dict, 12, 8).show()\n",
    "histogram(obs_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cb945",
   "metadata": {},
   "source": [
    "# Transform the column we want to consider to a SpaCy vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of series objects representing the columns of the new DataFrame  \n",
    "vector = nlp('tekst for vector').vector # create an arbitrary vector to be certain that we have the correct length\n",
    "vector_names = [f\"V{i}\" for i in range(len(vector))]  \n",
    "column_list = [] \n",
    "\n",
    "for name in vector_names:\n",
    "     column_list.append(pd.Series(name=name, index=df.index, dtype=float))  \n",
    "  \n",
    "# Loop over the strings in the original DataFrame and add their spaCy vectors to the column Series objects  \n",
    "for i, text in enumerate(df['obs']):  \n",
    "    doc = nlp(text)  \n",
    "    for j, value in enumerate(doc.vector):  \n",
    "        column_list[j][i] = value  \n",
    "  \n",
    "# Concatenate the column Series objects to create the new DataFrame  \n",
    "df_vec = pd.concat(column_list, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad3fa5",
   "metadata": {},
   "source": [
    "## Remove samples that have empty vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the norm of each row using np.linalg.norm()  \n",
    "norms = df_vec.apply(lambda row: np.linalg.norm(row), axis=1)  \n",
    "\n",
    "print(f'Number of samples before selection: {len(df)}')\n",
    "# Filter out the rows where the norm is zero  \n",
    "df_vec = df_vec[norms != 0]  \n",
    "\n",
    "print(f'Number of samples for clustering: {len(df_vec)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09486e9f",
   "metadata": {},
   "source": [
    "## Set Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = None\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba4a1e",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore the correlations in our data set \n",
    "plt.figure(figsize=(10,10))\n",
    "correlation = df_vec.corr()\n",
    "# Keep only the upper triangle of the correlation matrix  \n",
    "correlation = np.triu(correlation, k=1)  \n",
    "\n",
    "sns.heatmap(abs(correlation), center = 0, cmap=\"RdBu\", vmax = 1.0, vmin = 0.0)\n",
    "\n",
    "# The correlation measure used here is Pearson’s correlation. \n",
    "# In our case the lighter the square the stronger the correlation between two variables.\n",
    "\n",
    "# Print the maximum and minimum correlations  \n",
    "max_corr = correlation.max()  \n",
    "min_corr = correlation.min()\n",
    "avg_corr = abs(correlation).mean()\n",
    "print(f'Max correlation: {max_corr:.3f}')  \n",
    "print(f'Min correlation: {min_corr:.3f}')  \n",
    "print(f'Mean absolute correlation: {avg_corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7e95",
   "metadata": {},
   "source": [
    "# Affinity Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71f763",
   "metadata": {},
   "source": [
    "## Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_vec)\n",
    "\n",
    "dist_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        vector_i = df_vec.iloc[i]\n",
    "        vector_j = df_vec.iloc[j]\n",
    "        \n",
    "        similarity = np.dot(vector_i, vector_j) / (np.linalg.norm(vector_i) * np.linalg.norm(vector_j))\n",
    "        similarity = max(-1.0, min(1.0, similarity)) # to remove corner cases from rounding\n",
    "        # to create a distance that is 0 for equal cases (similarity = 1) and 2 for cases that are very far apart\n",
    "        dist_matrix[i][j] = dist_matrix[j][i] = 1 - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffde76a",
   "metadata": {},
   "source": [
    "## Hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for hyperparameter damping and results\n",
    "stepsize = 0.025\n",
    "\n",
    "damping = np.arange(0.5, 1.0, stepsize)\n",
    "af_results = pd.DataFrame(index=damping, columns=['Clusters', 'SC', 'VRC', 'DBI'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212901fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_damping = 0.5\n",
    "best_VRC = 0\n",
    "n_clusters = 0\n",
    "\n",
    "for index in af_results.index:\n",
    "    af = AffinityPropagation(\n",
    "        damping = index,\n",
    "        max_iter = 500,\n",
    "        affinity = 'precomputed',\n",
    "        verbose = False,\n",
    "        random_state = RANDOM_STATE).fit(dist_matrix)\n",
    "    \n",
    "    #cluster_centers_indices = af.cluster_centers_indices_\n",
    "    n_clust = len(af.cluster_centers_indices_)\n",
    "    af_results.loc[index]['Clusters'] = len(af.cluster_centers_indices_)\n",
    "    if n_clusters == 0:\n",
    "        n_clusters = n_clust\n",
    "        \n",
    "    if n_clust > 1:\n",
    "        VRC = metrics.calinski_harabasz_score(dist_matrix, af.labels_)\n",
    "        if VRC > best_VRC:\n",
    "            best_damping = index\n",
    "            best_VRC = VRC\n",
    "            best_aflabels = af.labels_\n",
    "            centers = af.cluster_centers_indices_\n",
    "            n_clusters = len(centers)\n",
    "    \n",
    "    \n",
    "        af_results.loc[index]['SC'] = metrics.silhouette_score(dist_matrix, af.labels_, metric=\"precomputed\")\n",
    "        af_results.loc[index]['VRC'] = metrics.calinski_harabasz_score(dist_matrix, af.labels_)\n",
    "        af_results.loc[index]['DBI'] = metrics.davies_bouldin_score(dist_matrix, af.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selected damping factor: {best_damping:0.3f}')\n",
    "\n",
    "SC = af_results.loc[best_damping]['SC']\n",
    "DBI = af_results.loc[best_damping]['DBI']\n",
    "\n",
    "print(f'Estimated number of clusters: {n_clusters}')\n",
    "if n_clusters > 1:\n",
    "\n",
    "    print(f'Silhouette Coefficient: {SC:0.3f}')\n",
    "    print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {best_VRC:0.3f}')\n",
    "    print(f'Davies-Bouldin Index: {DBI:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotplot for nr of cluster per dampening factor\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(af_results.index * 100, af_results['Clusters'], marker='.', linestyle='', markersize=10)  \n",
    "ax.set_xlabel('Dampening factor (%)')  \n",
    "ax.set_ylabel('Nr of clusters')  \n",
    "ax.set_title('Nr of clusters per dampening factor') \n",
    "ax.set_ylim(bottom=0, top = af_results['Clusters'].max() + 1)  \n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))  \n",
    "\n",
    "# Set x-axis labels for values between 50 to 100  \n",
    "ax.set_xticks(range(50, 101, 10))  \n",
    "ax.set_xticklabels([str(i) for i in range(50, 101, 10)])  \n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e185044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotplot for silhouette score per dampening factor\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(af_results.index * 100, af_results['SC'] * 100, marker='.', linestyle='', markersize=10)  \n",
    "ax.set_xlabel('Dampening factor (%)')  \n",
    "ax.set_ylabel('Silhouette score (%)')  \n",
    "ax.set_title('Silhouette score per dampening factor') \n",
    "ax.set_ylim(bottom=0, top = af_results['SC'].max()*100 + 1)  \n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))  \n",
    "\n",
    "# Set x-axis labels for values between 50 to 100  \n",
    "ax.set_xticks(range(50, 101, 10))  \n",
    "ax.set_xticklabels([str(i) for i in range(50, 101, 10)])  \n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958566ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotplot for VRC per dampening factor\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(af_results.index * 100, af_results['VRC'], marker='.', linestyle='', markersize=10)  \n",
    "ax.set_xlabel('Dampening factor (%)')  \n",
    "ax.set_ylabel('VRC')  \n",
    "ax.set_title('VRC per dampening factor') \n",
    "ax.set_ylim(bottom=0, top = af_results['VRC'].max() + 2)  \n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))  \n",
    "\n",
    "# Set x-axis labels for values between 50 to 100  \n",
    "ax.set_xticks(range(50, 101, 10))  \n",
    "ax.set_xticklabels([str(i) for i in range(50, 101, 10)])  \n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotplot for DBI per dampening factor\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(af_results.index * 100, af_results['DBI'], marker='.', linestyle='', markersize=10)  \n",
    "ax.set_xlabel('Dampening factor (%)')  \n",
    "ax.set_ylabel('DBI')  \n",
    "ax.set_title('DBI per dampening factor') \n",
    "ax.set_ylim(bottom=0, top = af_results['DBI'].max() + 2)  \n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))  \n",
    "\n",
    "# Set x-axis labels for values between 50 to 100  \n",
    "ax.set_xticks(range(50, 101, 10))  \n",
    "ax.set_xticklabels([str(i) for i in range(50, 101, 10)])  \n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dicts = {}\n",
    "\n",
    "for i, label in enumerate(best_aflabels):\n",
    "    words = df.loc[df_vec.index[i]]['obs']\n",
    "    \n",
    "    if label in label_dicts:\n",
    "        label_dicts[label] = word_count(words, label_dicts[label])\n",
    "    else:\n",
    "        dict = {}\n",
    "        label_dicts[label] = word_count(words, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ff9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordclouds(labels):\n",
    "    n = labels.max() + 1\n",
    "    label_dicts = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        words = df.loc[df_vec.index[i]]['obs']\n",
    "\n",
    "        if label in label_dicts:\n",
    "            label_dicts[label] = word_count(words, label_dicts[label])\n",
    "        else:\n",
    "            dict = {}\n",
    "            label_dicts[label] = word_count(words, dict)\n",
    "\n",
    "    wordclouds = [wordcloud_from_dict(value, 5, 5) for value in label_dicts.values()]\n",
    "\n",
    "    # Create a grid of subplots, ncols wide or less if there are less clusters\n",
    "    ncols = min(5, n)\n",
    "    nrows = int(np.ceil(len(wordclouds) / ncols))\n",
    "    nplots = ncols * nrows\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20))  \n",
    "    keys = list(label_dicts.keys())\n",
    "\n",
    "    # Plot each individual wordcloud in a separate subplot  \n",
    "    for i in range(nplots): \n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        if nrows == 1:\n",
    "            if i < len(wordclouds):\n",
    "                index = keys.index(i)\n",
    "                axs[col].imshow(wordclouds[index].to_array(), interpolation='bilinear')  \n",
    "                axs[col].set_title(f'Cluster {keys[index]}', pad = 15)\n",
    "            axs[col].axis('off')  \n",
    "        else:\n",
    "            if i < len(wordclouds):\n",
    "                index = keys.index(i)\n",
    "                axs[row, col].imshow(wordclouds[index].to_array(), interpolation='bilinear')  \n",
    "                axs[row, col].set_title(f'Cluster {keys[index]}')\n",
    "            axs[row, col].axis('off')  \n",
    "\n",
    "\n",
    "    # Show the grid of subplots  \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6920214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds(best_aflabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3531029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_size_hist(labels):\n",
    "    freq = {}\n",
    "    for label in labels:\n",
    "        if label in freq:\n",
    "            freq[label] += 1\n",
    "        else:\n",
    "            freq[label] = 1\n",
    "        \n",
    "    data = freq.values()\n",
    "    plt.hist(data)\n",
    "\n",
    "    # add labels and title to the chart  \n",
    "    plt.xlabel(\"Cluster size\")  \n",
    "    plt.ylabel(\"Frequency\")  \n",
    "    plt.title(\"Cluster size Frequency Histogram\")  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_plot(dist_matrix, labels):\n",
    "    n_clusters = labels.max() + 1\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = metrics.silhouette_samples(dist_matrix, labels, metric = 'precomputed')\n",
    "    min_sil = math.floor(min(sample_silhouette_values) / 0.2) * 0.2\n",
    "    max_sil = max(sample_silhouette_values)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    ax.set_xlim([min_sil, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette plots of individual clusters,\n",
    "    # to demarcate them clearly.    \n",
    "    ax.set_ylim([0, len(df_vec) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = metrics.silhouette_score(dist_matrix, labels, metric = 'precomputed')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(min_sil, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax.set_title(\"Silhouette plot for each cluster\")\n",
    "        ax.set_xlabel(\"Silhouette coefficient\")\n",
    "        ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        xticks = np.arange(min_sil, 1.1, 0.2)  \n",
    "        ax.set_xticks(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf20793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_size_hist(best_aflabels)\n",
    "silhouette_plot(dist_matrix, best_aflabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a393dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(best_aflabels+1)):\n",
    "    print(f'Cluster {i} size: {sum(best_aflabels == i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1840c5",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bd46d",
   "metadata": {},
   "source": [
    "## Affinity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e42402",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_matrix = 2 - dist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098b6fa",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for hyperparameter n_clusters and results\n",
    "n_clusters = range(2, 10)\n",
    "sc_results = pd.DataFrame(index=n_clusters, columns=['SC', 'VRC', 'DBI'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f29d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_VRC = 0\n",
    "n = 0\n",
    "\n",
    "for index in sc_results.index:\n",
    "    sc = SpectralClustering(\n",
    "        n_clusters = index,\n",
    "        random_state = RANDOM_STATE, \n",
    "        affinity = 'precomputed',\n",
    "        verbose = False,\n",
    "        assign_labels='discretize').fit(aff_matrix)\n",
    "            \n",
    "    VRC = metrics.calinski_harabasz_score(dist_matrix, sc.labels_)\n",
    "    if VRC > best_VRC:    \n",
    "        n = index\n",
    "        best_VRC = VRC\n",
    "        best_SC = SC\n",
    "        best_sclabels = sc.labels_\n",
    "        \n",
    "    sc_results.loc[index]['SC'] = metrics.silhouette_score(dist_matrix, sc.labels_, metric=\"precomputed\")\n",
    "    sc_results.loc[index]['VRC'] = metrics.calinski_harabasz_score(dist_matrix, sc.labels_)\n",
    "    sc_results.loc[index]['DBI'] = metrics.davies_bouldin_score(dist_matrix, sc.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce64563",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selected number of clusters: {n}')\n",
    "\n",
    "SC = sc_results.loc[n]['SC']\n",
    "DBI = sc_results.loc[n]['DBI']\n",
    "print(f'Silhouette Coefficient: {SC:0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {best_VRC:0.3f}')\n",
    "print(f'Davies-Bouldin Index: {DBI:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983549f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds(best_sclabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44244ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_size_hist(best_sclabels)\n",
    "silhouette_plot(dist_matrix, best_sclabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27314e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(best_sclabels+1)):\n",
    "    print(f'Cluster {i} size: {sum(best_sclabels == i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52424a",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for hyperparameter damping and results\n",
    "n_clusters = range(2, 20)\n",
    "ac_results = pd.DataFrame(index=n_clusters, columns=['SC', 'VRC', 'DBI'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01676a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_VRC = 0\n",
    "n = 0\n",
    "\n",
    "for index in ac_results.index:\n",
    "    ac = AgglomerativeClustering(\n",
    "        n_clusters = index,\n",
    "        metric = 'precomputed',\n",
    "        linkage = 'average').fit(dist_matrix)\n",
    "            \n",
    "    VRC = metrics.calinski_harabasz_score(dist_matrix, ac.labels_)\n",
    "    if VRC > best_VRC:\n",
    "        n = index\n",
    "        best_VRC = VRC\n",
    "        best_aclabels = ac.labels_\n",
    "        \n",
    "    ac_results.loc[index]['SC'] = metrics.silhouette_score(dist_matrix, ac.labels_, metric=\"precomputed\")\n",
    "    ac_results.loc[index]['VRC'] = metrics.calinski_harabasz_score(dist_matrix, ac.labels_)\n",
    "    ac_results.loc[index]['DBI'] = metrics.davies_bouldin_score(dist_matrix, ac.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ce3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selected number of clusters: {n}')\n",
    "\n",
    "SC = ac_results.loc[n]['SC']\n",
    "DBI = ac_results.loc[n]['DBI']\n",
    "print(f'Silhouette Coefficient: {SC:0.3f}')\n",
    "print(f'Calinski-Harabasz Index / Variance Ratio Criterion: {best_VRC:0.3f}')\n",
    "print(f'Davies-Bouldin Index: {DBI:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_size_hist(best_aclabels)\n",
    "plot_wordclouds(best_aclabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d275a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot(dist_matrix, best_aclabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec980e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(best_aclabels)+1):\n",
    "    print(f'Cluster {i} size: {sum(best_aclabels == i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8411ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9963eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['obs'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1997e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sclabels[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cad3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
